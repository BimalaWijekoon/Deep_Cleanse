{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nimport matplotlib\nmatplotlib.use('Agg')  # Set backend to avoid display issues\nimport matplotlib.pyplot as plt\nimport urllib.request\nimport tarfile\nimport zipfile\nfrom pathlib import Path\nimport glob\nimport argparse\nimport re\nimport shutil\nimport sys\nimport librosa\nimport soundfile as sf\n\n# Handle Colab vs. Kaggle environment differences\nIN_COLAB = 'google.colab' in sys.modules\nIN_KAGGLE = 'kaggle_secrets' in sys.modules if not IN_COLAB else False\n\nif IN_COLAB:\n    # Colab directories\n    BASE_DIR = Path('/content')\n    MODEL_DIR = BASE_DIR / 'models'\n    LOG_DIR = BASE_DIR / 'logs'\n    WORKING_DIR = BASE_DIR / 'working'\nelse:\n    # Kaggle directories - updated to use /kaggle/working for outputs\n    BASE_DIR = Path('/kaggle')\n    WORKING_DIR = BASE_DIR / 'working'\n    MODEL_DIR = WORKING_DIR / 'models'\n    LOG_DIR = WORKING_DIR / 'logs'\n\n# Create directories\nWORKING_DIR.mkdir(exist_ok=True, parents=True)\nMODEL_DIR.mkdir(exist_ok=True, parents=True)\nLOG_DIR.mkdir(exist_ok=True, parents=True)\n\n# Configuration\nSAMPLE_RATE = 16000  # Standard speech sampling rate\nFRAME_LENGTH = 2048  # Length of audio frames\nHOP_LENGTH = 512     # Hop length between frames\n\ndef setup_arg_parser():\n    \"\"\"Setup argument parser for training configuration\"\"\"\n    parser = argparse.ArgumentParser(description='Train an audio denoising autoencoder')\n    parser.add_argument('--resume-from', type=str, default=None, \n                        help='Checkpoint file to resume training from')\n    parser.add_argument('--keep-checkpoints', type=int, default=3, \n                        help='Number of recent checkpoints to keep')\n    parser.add_argument('--epochs', type=int, default=100, \n                        help='Number of epochs to train')\n    parser.add_argument('--batch-size', type=int, default=32, \n                        help='Batch size for training')\n    \n    return parser.parse_known_args()[0]\n\ndef setup_gpus():\n    \"\"\"Configure TensorFlow to use multiple GPUs if available\"\"\"\n    gpus = tf.config.list_physical_devices('GPU')\n    \n    if not gpus:\n        print(\"No GPUs found. Running on CPU.\")\n        return False\n    \n    print(f\"Found {len(gpus)} GPU(s):\")\n    for gpu in gpus:\n        print(f\"  - {gpu.name}\")\n    \n    # Multi-GPU strategy\n    if len(gpus) > 1:\n        strategy = tf.distribute.MirroredStrategy()\n        print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} devices\")\n        return strategy\n    else:\n        print(\"Using default strategy (single GPU)\")\n        return tf.distribute.get_strategy()\n\ndef build_audio_denoising_autoencoder(strategy=None, input_shape=(128, 128, 1)):\n    \"\"\"\n    Build a professional-level 2D CNN-based audio denoising autoencoder\n    with multi-GPU support if available\n    \"\"\"\n    if strategy:\n        with strategy.scope():\n            inputs = layers.Input(shape=input_shape)\n            \n            # Encoder\n            x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n            x = layers.BatchNormalization()(x)\n            x = layers.MaxPooling2D((2, 2))(x)\n            \n            x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.MaxPooling2D((2, 2))(x)\n            \n            x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.MaxPooling2D((2, 2))(x)\n            \n            # Bottleneck\n            x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            \n            # Decoder\n            x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n            x = layers.BatchNormalization()(x)\n            \n            x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n            x = layers.BatchNormalization()(x)\n            \n            x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n            x = layers.BatchNormalization()(x)\n            \n            # Output layer\n            outputs = layers.Conv2D(1, (1, 1), activation='linear')(x)\n            \n            model = models.Model(inputs, outputs)\n            model.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n                loss='mse',\n                metrics=['mae']\n            )\n    else:\n        # Standard model creation without strategy\n        inputs = layers.Input(shape=input_shape)\n        \n        # Encoder (similar structure as with strategy)\n        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        \n        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        \n        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        \n        # Bottleneck\n        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        \n        # Decoder\n        x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        # Output layer\n        outputs = layers.Conv2D(1, (1, 1), activation='linear')(x)\n        \n        model = models.Model(inputs, outputs)\n        model.compile(\n            optimizer='adam',\n            loss='mse',\n            metrics=['mae']\n        )\n    \n    return model\n\ndef download_vctk_dataset():\n    \"\"\"\n    Use VCTK dataset from Kaggle input directory\n    \"\"\"\n    # Predefined path for Kaggle input dataset\n    input_path = Path('/kaggle/input/vctk-corpus/VCTK-Corpus')\n    \n    # Check if dataset exists\n    if not input_path.exists():\n        raise ValueError(f\"VCTK dataset not found at {input_path}. Please ensure the dataset is uploaded.\")\n    \n    print(f\"Using VCTK dataset from: {input_path}\")\n    return input_path\n    \ndef prepare_audio_data(data_path, target_sr=SAMPLE_RATE, max_files=500, spec_height=128, spec_width=128, batch_size=32):\n    \"\"\"\n    Memory-efficient audio data preparation with generator-based processing\n    \"\"\"\n    wav_path = Path(data_path) / 'wav48'\n    \n    def resize_spectrogram(spec, target_height, target_width):\n        \"\"\"\n        Resize spectrogram to a consistent shape with robust handling\n        \"\"\"\n        # Ensure 2D input\n        if spec.ndim > 2:\n            spec = spec.squeeze()\n        \n        # Truncate or pad height\n        if spec.shape[0] > target_height:\n            spec = spec[:target_height, :]\n        else:\n            pad_height = target_height - spec.shape[0]\n            spec = np.pad(\n                spec, \n                ((0, pad_height), (0, 0)), \n                mode='constant', \n                constant_values=0\n            )\n        \n        # Truncate or pad width\n        if spec.shape[1] > target_width:\n            spec = spec[:, :target_width]\n        else:\n            pad_width = target_width - spec.shape[1]\n            spec = np.pad(\n                spec, \n                ((0, 0), (0, pad_width)), \n                mode='constant', \n                constant_values=0\n            )\n        \n        return spec\n    \n    def spectrogram_generator(file_list, noise_factor=0.05):\n        \"\"\"\n        Generator to process spectrograms in memory-efficient batches\n        \"\"\"\n        for file_path in file_list:\n            try:\n                # Load audio file\n                audio, sr = librosa.load(str(file_path), sr=target_sr)\n                \n                # Add noise\n                noisy_audio = audio + noise_factor * np.random.normal(0, 1, len(audio))\n                \n                # Compute spectrograms\n                clean_spec = np.abs(librosa.stft(audio, n_fft=FRAME_LENGTH, hop_length=HOP_LENGTH))\n                noisy_spec = np.abs(librosa.stft(noisy_audio, n_fft=FRAME_LENGTH, hop_length=HOP_LENGTH))\n                \n                # Convert to decibel scale\n                clean_spec = librosa.amplitude_to_db(clean_spec, ref=np.max)\n                noisy_spec = librosa.amplitude_to_db(noisy_spec, ref=np.max)\n                \n                # Normalize\n                clean_spec = (clean_spec - clean_spec.min()) / (clean_spec.max() - clean_spec.min())\n                noisy_spec = (noisy_spec - noisy_spec.min()) / (noisy_spec.max() - noisy_spec.min())\n                \n                # Resize and prepare spectrograms\n                clean_spec_resized = resize_spectrogram(clean_spec.T, spec_height, spec_width)\n                noisy_spec_resized = resize_spectrogram(noisy_spec.T, spec_height, spec_width)\n                \n                yield (\n                    noisy_spec_resized[np.newaxis, :, :, np.newaxis], \n                    clean_spec_resized[np.newaxis, :, :, np.newaxis]\n                )\n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n    \n    # Find all wav files in speaker subdirectories\n    wav_files = []\n    for speaker_dir in wav_path.iterdir():\n        if speaker_dir.is_dir():\n            wav_files.extend(list(speaker_dir.glob('*.wav')))\n    \n    print(f\"Total audio files found: {len(wav_files)}\")\n    \n    # Limit dataset size\n    wav_files = wav_files[:max_files]\n    \n    # Randomly shuffle files\n    np.random.shuffle(wav_files)\n    \n    # Split into training and validation sets\n    split_ratio = 0.8\n    split_idx = int(len(wav_files) * split_ratio)\n    \n    train_files = wav_files[:split_idx]\n    val_files = wav_files[split_idx:]\n    \n    # Create generators\n    train_generator = spectrogram_generator(train_files)\n    val_generator = spectrogram_generator(val_files)\n    \n    # Collect batches\n    def collect_batches(generator, batch_size):\n        noisy_batch = []\n        clean_batch = []\n        \n        for noisy, clean in generator:\n            noisy_batch.append(noisy)\n            clean_batch.append(clean)\n            \n            if len(noisy_batch) == batch_size:\n                yield np.concatenate(noisy_batch), np.concatenate(clean_batch)\n                noisy_batch = []\n                clean_batch = []\n        \n        # Handle remaining samples\n        if noisy_batch:\n            yield np.concatenate(noisy_batch), np.concatenate(clean_batch)\n    \n    # Collect training and validation data\n    train_data = list(collect_batches(train_generator, batch_size))\n    val_data = list(collect_batches(val_generator, batch_size))\n    \n    # Combine batches\n    train_noisy = np.concatenate([batch[0] for batch in train_data])\n    train_clean = np.concatenate([batch[1] for batch in train_data])\n    val_noisy = np.concatenate([batch[0] for batch in val_data])\n    val_clean = np.concatenate([batch[1] for batch in val_data])\n    \n    print(f\"Training spectrograms shape: {train_noisy.shape}\")\n    print(f\"Validation spectrograms shape: {val_noisy.shape}\")\n    \n    # Optional: save preprocessed data\n    np.save(WORKING_DIR / 'train_noisy_specs.npy', train_noisy)\n    np.save(WORKING_DIR / 'train_clean_specs.npy', train_clean)\n    np.save(WORKING_DIR / 'val_noisy_specs.npy', val_noisy)\n    np.save(WORKING_DIR / 'val_clean_specs.npy', val_clean)\n    \n    return train_noisy, train_clean, val_noisy, val_clean\ndef check_for_existing_data():\n    \"\"\"Check if preprocessed data already exists\"\"\"\n    data_paths = [\n        WORKING_DIR / 'train_noisy_specs.npy',\n        WORKING_DIR / 'train_clean_specs.npy',\n        WORKING_DIR / 'val_noisy_specs.npy',\n        WORKING_DIR / 'val_clean_specs.npy'\n    ]\n    \n    if all(path.exists() for path in data_paths):\n        print(\"Found preprocessed spectrograms\")\n        train_noisy = np.load(data_paths[0])\n        train_clean = np.load(data_paths[1])\n        val_noisy = np.load(data_paths[2])\n        val_clean = np.load(data_paths[3])\n        return train_noisy, train_clean, val_noisy, val_clean\n    \n    return None, None, None, None\n\ndef train_model(model, train_noisy, train_clean, val_noisy, val_clean, args):\n    \"\"\"\n    Train the audio denoising model with checkpoint management\n    \"\"\"\n    epochs = args.epochs\n    batch_size = args.batch_size\n    \n    # Adjust batch size for multi-GPU\n    gpus = len(tf.config.list_physical_devices('GPU'))\n    if gpus > 1:\n        batch_size = max(batch_size, 32 * gpus)\n        batch_size = batch_size - (batch_size % gpus) if batch_size % gpus != 0 else batch_size\n    \n    # Callbacks\n    checkpoint_callback = ModelCheckpoint(\n        filepath=MODEL_DIR / \"audio_denoiser.{epoch:02d}-{val_loss:.4f}.weights.h5\",\n        save_weights_only=True,\n        save_best_only=True,\n        monitor='val_loss'\n    )\n    \n    tensorboard_callback = TensorBoard(\n        log_dir=LOG_DIR,\n        histogram_freq=1,\n        write_graph=True,\n        update_freq='epoch'\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True\n    )\n    \n    # Checkpoint management callback\n    checkpoint_manager_callback = CheckpointManagerCallback(\n        args.keep_checkpoints,\n        args.resume_from\n    )\n    \n    history = model.fit(\n        train_noisy, train_clean,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(val_noisy, val_clean),\n        callbacks=[\n            checkpoint_callback, \n            early_stopping, \n            tensorboard_callback,\n            checkpoint_manager_callback\n        ]\n    )\n    \n    # Save final model\n    model.save(MODEL_DIR / \"audio_denoiser_final.keras\")\n    model.save(MODEL_DIR / \"audio_denoiser_final.h5\")\n    \n    # Save training history\n    np.save(MODEL_DIR / 'training_history.npy', history.history)\n    \n    # Plot training curves\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['mae'])\n    plt.plot(history.history['val_mae'])\n    plt.title('Mean Absolute Error')\n    plt.xlabel('Epoch')\n    plt.ylabel('MAE')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    \n    plt.tight_layout()\n    plt.savefig(MODEL_DIR / 'training_curves.png')\n    \n    return model, history\n\ndef test_model(model):\n    \"\"\"\n    Test the audio denoising model with a sample spectrogram\n    \"\"\"\n    # Create a simple test spectrogram\n    test_spec = np.random.random((1, 128, 128, 1))\n    \n    # Add noise\n    noisy_spec = test_spec + 0.1 * np.random.normal(0, 1, test_spec.shape)\n    \n    # Predict denoised spectrogram\n    denoised_spec = model.predict(noisy_spec)\n    \n    # Visualize results\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.imshow(test_spec[0, :, :, 0], cmap='viridis')\n    plt.title('Original')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(noisy_spec[0, :, :, 0], cmap='viridis')\n    plt.title('Noisy')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(denoised_spec[0, :, :, 0], cmap='viridis')\n    plt.title('Denoised')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(MODEL_DIR / 'test_denoising_results.png')\n    print(\"Test results saved to test_denoising_results.png\")\n\ndef check_for_checkpoint(initial_checkpoint=None, strategy=None):\n    \"\"\"\n    Check and load checkpoint if available\n    \"\"\"\n    if initial_checkpoint:\n        try:\n            model, initial_epoch = load_model_from_checkpoint(initial_checkpoint, strategy)\n            return model, initial_epoch\n        except Exception as e:\n            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n    \n    return create_and_save_model(strategy), 0\n\ndef create_and_save_model(strategy=None):\n    \"\"\"Create and save initial model\"\"\"\n    model = build_audio_denoising_autoencoder(strategy)\n    model.summary()\n    \n    # Save model architecture\n    model_json = model.to_json()\n    with open(MODEL_DIR / \"audio_denoiser_architecture.json\", \"w\") as json_file:\n        json_file.write(model_json)\n    \n    model.save_weights(MODEL_DIR / \"audio_denoiser_initial.weights.h5\")\n    model.save(MODEL_DIR / \"audio_denoiser_initial.keras\")\n    model.save(MODEL_DIR / \"audio_denoiser_initial.h5\")\n    \n    return model\n\ndef load_model_from_checkpoint(checkpoint_path, strategy=None):\n    \"\"\"Load model from checkpoint\"\"\"\n    model = build_audio_denoising_autoencoder(strategy)\n    model.load_weights(checkpoint_path)\n    \n    # Extract initial epoch\n    epoch_match = re.search(r'\\.(\\d+)-', os.path.basename(checkpoint_path))\n    initial_epoch = int(epoch_match.group(1)) if epoch_match else 0\n    \n    return model, initial_epoch\n\ndef manage_checkpoints(keep_count=3, started_checkpoint=None):\n    \"\"\"Manage model checkpoints\"\"\"\n    checkpoint_pattern = str(MODEL_DIR / \"audio_denoiser.*.weights.h5\")\n    checkpoints = glob.glob(checkpoint_pattern)\n    \n    if len(checkpoints) <= keep_count:\n        return\n    \n    checkpoint_info = []\n    for cp in checkpoints:\n        if started_checkpoint and os.path.basename(cp) == os.path.basename(started_checkpoint):\n            continue\n        \n        epoch_match = re.search(r'\\.(\\d+)-', os.path.basename(cp))\n        if epoch_match:\n            epoch = int(epoch_match.group(1))\n            checkpoint_info.append((cp, epoch))\n    \n    checkpoint_info.sort(key=lambda x: x[1], reverse=True)\n    \n    # Delete older checkpoints\n    for cp, _ in checkpoint_info[keep_count:]:\n        os.remove(cp)\n\nclass CheckpointManagerCallback(tf.keras.callbacks.Callback):\n    \"\"\"Callback to manage checkpoints during training\"\"\"\n    def __init__(self, keep_count, started_checkpoint):\n        super().__init__()\n        self.keep_count = keep_count\n        self.started_checkpoint = started_checkpoint\n    \n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 5 == 0:\n            manage_checkpoints(self.keep_count, self.started_checkpoint)\n\ndef main():\n    print(\"Starting Audio Denoising Autoencoder setup...\")\n    \n    # Parse arguments\n    args = setup_arg_parser()\n    \n    # Setup GPU strategy\n    strategy = setup_gpus()\n    \n    # Check for existing preprocessed data\n    train_noisy, train_clean, val_noisy, val_clean = check_for_existing_data()\n    \n    if train_noisy is None:\n        # Download dataset\n        data_path = download_vctk_dataset()\n        print(f\"Dataset available at: {data_path}\")\n        \n        # Prepare data\n        train_noisy, train_clean, val_noisy, val_clean = prepare_audio_data(data_path)\n    \n    # Check for checkpoint and create/load model\n    model, initial_epoch = check_for_checkpoint(args.resume_from, strategy)\n    \n    # Train model\n    model, history = train_model(\n        model, \n        train_noisy, train_clean, \n        val_noisy, val_clean, \n        args\n    )\n    \n    # Test model\n    test_model(model)\n    \n    # Final checkpoint management\n    manage_checkpoints(args.keep_checkpoints, args.resume_from)\n    \n    print(\"\\nSetup complete!\")\n    print(f\"Model files saved to: {MODEL_DIR}\")\n    print(f\"Logs saved to: {LOG_DIR}\")\n\nif __name__ == \"__main__\":\n    # Set memory growth for GPUs\n    physical_devices = tf.config.list_physical_devices('GPU')\n    for device in physical_devices:\n        try:\n            tf.config.experimental.set_memory_growth(device, True)\n        except Exception as e:\n            print(f\"Could not set memory growth for {device}: {e}\")\n    \n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T06:15:56.972012Z","iopub.execute_input":"2025-03-25T06:15:56.972353Z","iopub.status.idle":"2025-03-25T06:16:51.894466Z","shell.execute_reply.started":"2025-03-25T06:15:56.972331Z","shell.execute_reply":"2025-03-25T06:16:51.893573Z"}},"outputs":[{"name":"stdout","text":"Starting Audio Denoising Autoencoder setup...\nFound 2 GPU(s):\n  - /physical_device:GPU:0\n  - /physical_device:GPU:1\nUsing MirroredStrategy with 2 devices\nUsing VCTK dataset from: /kaggle/input/vctk-corpus/VCTK-Corpus\nDataset available at: /kaggle/input/vctk-corpus/VCTK-Corpus\nTotal audio files found: 44242\nTraining spectrograms shape: (400, 128, 128, 1)\nValidation spectrograms shape: (100, 128, 128, 1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m295,040\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_1 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m73,792\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_2 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m18,464\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m33\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_transpose_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m777,985\u001b[0m (2.97 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">777,985</span> (2.97 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m776,577\u001b[0m (2.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">776,577</span> (2.96 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 549ms/step - loss: 1.2449 - mae: 0.8630 - val_loss: 0.1578 - val_mae: 0.3054\nEpoch 2/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - loss: 0.6986 - mae: 0.6568 - val_loss: 0.1589 - val_mae: 0.3074\nEpoch 3/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - loss: 0.4038 - mae: 0.5055 - val_loss: 0.1598 - val_mae: 0.3091\nEpoch 4/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - loss: 0.2654 - mae: 0.4124 - val_loss: 0.1609 - val_mae: 0.3114\nEpoch 5/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - loss: 0.1841 - mae: 0.3460 - val_loss: 0.1638 - val_mae: 0.3161\nEpoch 6/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - loss: 0.1479 - mae: 0.3120 - val_loss: 0.1668 - val_mae: 0.3206\nEpoch 7/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 0.1327 - mae: 0.2981 - val_loss: 0.1703 - val_mae: 0.3257\nEpoch 8/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - loss: 0.1199 - mae: 0.2851 - val_loss: 0.1740 - val_mae: 0.3311\nEpoch 9/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - loss: 0.1097 - mae: 0.2750 - val_loss: 0.1777 - val_mae: 0.3362\nEpoch 10/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 0.1016 - mae: 0.2656 - val_loss: 0.1812 - val_mae: 0.3410\nEpoch 11/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - loss: 0.0954 - mae: 0.2578 - val_loss: 0.1846 - val_mae: 0.3455\nEpoch 12/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 0.0919 - mae: 0.2538 - val_loss: 0.1874 - val_mae: 0.3492\nEpoch 13/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - loss: 0.0880 - mae: 0.2484 - val_loss: 0.1900 - val_mae: 0.3526\nEpoch 14/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 0.0857 - mae: 0.2461 - val_loss: 0.1924 - val_mae: 0.3557\nEpoch 15/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step - loss: 0.0841 - mae: 0.2437 - val_loss: 0.1945 - val_mae: 0.3582\nEpoch 16/100\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step - loss: 0.0788 - mae: 0.2355 - val_loss: 0.1965 - val_mae: 0.3607\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\nTest results saved to test_denoising_results.png\n\nSetup complete!\nModel files saved to: /kaggle/working/models\nLogs saved to: /kaggle/working/logs\n","output_type":"stream"}],"execution_count":7}]}